{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZJhtSc1yDkx"
   },
   "source": [
    "# Notebook 3 Sentiment Analysis: Deep Dive into Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1ovvYA6yLv7"
   },
   "source": [
    "write some intro regarding the notebook, discussing about various classification models in ML and it's implementation in the notebook that we are going perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8D6fgOmyZck"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ucEpb7ZRyLH8"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# libraries to interact with files and system\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning libraries using sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ifywAoPx9nk"
   },
   "outputs": [],
   "source": [
    "# new library import\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB-dI3udy5qQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We will copy the data that we have already pre-processed in the pervious notebook and custom Word2Vec models(CBOW_Word2Vec and SKIP_Word2Vec) that we have generated in Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8EUpir4By4wK"
   },
   "outputs": [],
   "source": [
    "# Copying the data from drive to local repo\n",
    "!mkdir \"/content/Dataset/\"\n",
    "!mkdir \"/content/Models/\"\n",
    "!cp \"/content/drive/MyDrive/Twitter_Sentiment_Analysis_1/training_processed_data.csv\" \"/content/Dataset/\"\n",
    "!cp \"/content/drive/MyDrive/Twitter_Sentiment_Analysis_1/CBOW_Word2Vec.model\" \"/content/Models/\"\n",
    "!cp \"/content/drive/MyDrive/Twitter_Sentiment_Analysis_1/SKIP_Word2Vec.model\" \"/content/Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xp_sYE7s1RjV",
    "outputId": "4de06abe-cf95-48ff-b4db-219e81b8f2d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687662</th>\n",
       "      <td>0</td>\n",
       "      <td>finally caved hell finally frozen cleaning ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135478</th>\n",
       "      <td>1</td>\n",
       "      <td>well mind one umm taco bell ordar yeah ok fav ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603255</th>\n",
       "      <td>0</td>\n",
       "      <td>lost stressed wish i sat nav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561390</th>\n",
       "      <td>1</td>\n",
       "      <td>louis memphis party bttw girl june</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674255</th>\n",
       "      <td>0</td>\n",
       "      <td>buggy much</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "687662        0  finally caved hell finally frozen cleaning ant...\n",
       "1135478       1  well mind one umm taco bell ordar yeah ok fav ...\n",
       "603255        0                       lost stressed wish i sat nav\n",
       "1561390       1                 louis memphis party bttw girl june\n",
       "674255        0                                         buggy much"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data into memory\n",
    "dataset = pd.read_csv(\"./Dataset/training_processed_data.csv\")\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziLnAtjaNAdu"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "2LDr0nLX5bfM",
    "outputId": "3c536284-7b40-43cf-c94c-da59e1f47dcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target        0\n",
       "text      27373\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values in the dataset\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "XX8cja_F5f_r",
    "outputId": "e5679d10-34bb-4730-cd19-56af5e254fcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remocing null values in the dataset\n",
    "dataset.dropna(how = 'any', axis = 0, inplace = True)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xGQpRYU3oXI",
    "outputId": "178ddde2-6f24-4dcf-f6d5-278becd548ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique value in target feature\n",
    "dataset['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdpZFptC14Yw"
   },
   "source": [
    "Load our custom Word2Vec models that we have created in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OpRmoxKy2GyM"
   },
   "outputs": [],
   "source": [
    "# importing library\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\piyush\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting numpy<2.0,>=1.18.5\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.0\n",
      "    Uninstalling gensim-4.3.0:\n",
      "      Successfully uninstalled gensim-4.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Piyush\\\\anaconda3\\\\Lib\\\\site-packages\\\\~ensim\\\\corpora\\\\_mmreader.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\piyush\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.2-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Piyush\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nVGbdGBb1p38"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m CBOW_Word2Vec \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Models/CBOW_Word2Vec.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[1;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \n\u001b[0;32m   1940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1942\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Word2Vec, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[0;32m   1944\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1461\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "CBOW_Word2Vec = Word2Vec.load(\"./Models/CBOW_Word2Vec.model\")\n",
    "# SKIP_Word2Vec = Word2Vec.load(\"./Models/SKIP_Word2Vec.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "allXLoDu39_K"
   },
   "source": [
    "## Splitting the data\n",
    "\n",
    "We will be splitting the data into three groups\n",
    "1. Training Data\n",
    "2. Validation Data\n",
    "3. Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Aautbssd4H8N"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DeFc7fsj4NIa"
   },
   "outputs": [],
   "source": [
    "# sliptting the data\n",
    "\n",
    "X_Temp, X_test, y_Temp, y_test = train_test_split(dataset['text'], dataset['target'], test_size = 0.1, random_state = 42)\n",
    "\n",
    "X_Train, X_val, y_train, y_val = train_test_split(X_Temp,y_Temp, test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8LPlgrw4tCH",
    "outputId": "eff768b6-a766-4b26-de37-a17876a9761c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132291, 283073, 157263)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of the data\n",
    "len(X_Train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Vh-eVppUGHuq",
    "outputId": "a6440962-e5ae-4019-f93a-4eb56d16b0ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1034783</th>\n",
       "      <td>idk cant sleep im going regret tomorrow im bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418370</th>\n",
       "      <td>done gotta prepare another shoot tomorrow next...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622958</th>\n",
       "      <td>i complelty ignored twitter day twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505920</th>\n",
       "      <td>twitterific screwed phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397416</th>\n",
       "      <td>i hate really wish another choice canadian iph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "1034783    idk cant sleep im going regret tomorrow im bre...\n",
       "418370     done gotta prepare another shoot tomorrow next...\n",
       "622958               i complelty ignored twitter day twitter\n",
       "505920                             twitterific screwed phone\n",
       "397416     i hate really wish another choice canadian iph...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPoHzvrS7-QG"
   },
   "source": [
    "## Transforming Tweets into Vector Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F0HwO0-tH88a"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6aWUlua1Ncf4"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "ws_tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kfoX8PjZGtIH"
   },
   "outputs": [],
   "source": [
    "def tweet_to_vec(tweets,model, vector_size = 300):\n",
    "  def process_tweet(tweet):\n",
    "    words = ws_tokenizer.tokenize(tweet)\n",
    "    words = [word for word in words if word in model.wv]\n",
    "\n",
    "    if len(words) == 0:\n",
    "      return np.zeros(vector_size)\n",
    "    return model.wv.get_mean_vector(words)\n",
    "\n",
    "  vectors = tweets.apply(process_tweet)\n",
    "  X = np.vstack(vectors)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9gpd_U3_HZk",
    "outputId": "8e7982fc-8b02-4f6d-9943-80b389d6c2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "# Experiment\n",
    "temp_1 = X_Train[:10]\n",
    "test = tweet_to_vec(temp_1, CBOW_Word2Vec)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Pb-eDLNQgoB",
    "outputId": "ed48b16a-0bea-40ee-e738-2ea13d7f2555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "[-2.17451584e-02  4.95528523e-03 -1.04022145e-01 -5.18418988e-03\n",
      "  9.15786400e-02 -2.27296930e-02  9.50797834e-03  1.36503140e-02\n",
      " -3.54471020e-02 -1.36323914e-01 -4.13204692e-02 -6.83616921e-02\n",
      " -9.27407369e-02 -6.26861528e-02  2.57588755e-02  8.47513899e-02\n",
      "  3.66605632e-02 -3.61013450e-02  7.46349320e-02 -1.12934615e-02\n",
      "  6.78092334e-03  7.30573013e-02 -1.34074278e-02  2.10939199e-02\n",
      "  2.65330598e-02 -2.06592456e-02  9.09490809e-02 -7.79620605e-03\n",
      "  7.42443204e-02  6.11015894e-02 -3.84232961e-02 -2.98781004e-02\n",
      "  4.11007963e-02 -9.51212719e-02  9.20995250e-02 -1.47104515e-02\n",
      "  5.29062189e-02  8.12230445e-03 -2.13759094e-02 -9.78678837e-02\n",
      "  5.54849729e-02  4.69025336e-02 -7.83312321e-03 -3.76410373e-02\n",
      "  6.49833381e-02 -2.92696860e-02  8.73114243e-02 -2.84018461e-02\n",
      " -5.64337429e-03  1.73620451e-02  6.21026903e-02 -5.95164998e-03\n",
      "  1.01328395e-01 -4.95423079e-02  3.64060700e-02 -6.26948923e-02\n",
      "  1.41417328e-02 -6.43068030e-02  2.71275789e-02 -5.51883876e-02\n",
      " -2.24994104e-02 -3.14890891e-02  4.52103950e-02 -7.17187971e-02\n",
      "  4.73080715e-03  1.17736720e-02 -1.31552517e-01 -4.28579226e-02\n",
      "  3.96432765e-02  4.35831957e-02  1.12364851e-02  1.89461391e-02\n",
      " -1.55043101e-03 -7.21895173e-02  1.12737648e-01  1.64379179e-02\n",
      " -6.59356639e-02 -3.52526866e-02 -1.43262399e-02 -4.92311865e-02\n",
      " -4.77114916e-02  1.63471885e-02  5.69038503e-02  3.38373408e-02\n",
      "  1.03687510e-01  1.88868474e-02 -3.32215726e-02  2.67680865e-02\n",
      " -5.71786985e-02  6.75295442e-02 -1.58948768e-02  5.74812442e-02\n",
      " -4.80557010e-02  1.96010545e-02  1.37994522e-02  9.90640894e-02\n",
      "  1.25406086e-02 -8.60551298e-02  4.45399880e-02 -7.98715875e-02\n",
      " -1.02868848e-01 -1.06177235e-03 -1.64775982e-01 -4.40675318e-02\n",
      " -1.25119053e-02  6.83506429e-02  3.82272042e-02  7.89573565e-02\n",
      "  1.28427535e-01  1.85859472e-01 -2.03976613e-02  1.79265556e-03\n",
      "  6.48531839e-02 -1.07825398e-02  8.34026635e-02 -1.03875503e-01\n",
      " -8.40920135e-02  1.32261708e-01  9.38369185e-02  6.04479499e-02\n",
      " -6.22865781e-02 -1.97523758e-02  1.31610274e-01 -2.50062272e-02\n",
      "  9.14716497e-02 -1.48252565e-02 -1.18377008e-01 -2.40009706e-02\n",
      "  1.29598588e-01  1.59541070e-02 -1.88795235e-02 -7.89515525e-02\n",
      "  6.30844533e-02  3.12969387e-02  8.74375459e-03 -3.85057852e-02\n",
      " -3.84431705e-02 -1.19476309e-02 -5.99026717e-02  3.78734432e-03\n",
      " -7.58797377e-02  1.51170110e-02  4.35925573e-02 -1.98484994e-02\n",
      " -2.37854347e-02 -3.47935595e-02  9.40237194e-02  3.73569950e-02\n",
      " -3.75462919e-02 -4.78121974e-02  1.11587867e-01  1.15855755e-02\n",
      " -3.16078290e-02  1.08549399e-02  6.41698316e-02 -1.27682751e-02\n",
      "  6.31785914e-02  2.78626867e-02  3.96586023e-02 -1.68856781e-03\n",
      "  1.76979676e-02 -3.02490480e-02 -4.30156887e-02  8.95426050e-02\n",
      " -5.47612384e-02  3.34960856e-02  7.74694383e-02 -2.58715767e-02\n",
      " -2.08869334e-02  2.24319249e-02 -4.83724289e-02  4.74428572e-02\n",
      " -9.83511209e-02  3.66904438e-02 -5.92238605e-02 -3.81731354e-02\n",
      "  3.21783088e-02  7.92665780e-02 -1.04244091e-02  2.54651643e-02\n",
      "  8.21450874e-02 -5.55424057e-02  6.96361363e-02  4.66007404e-02\n",
      "  7.35137314e-02 -4.19288874e-02 -4.99808602e-02  5.52285612e-02\n",
      " -1.24912383e-02  6.27106875e-02 -2.21457034e-02  1.86869130e-02\n",
      "  5.59674576e-02 -1.99168306e-02  3.12929042e-02 -5.31321838e-02\n",
      "  3.45306955e-02  5.15367687e-02 -1.40309915e-01  5.10165691e-02\n",
      "  6.31944835e-02 -8.57030153e-02 -1.84638873e-02 -7.16855824e-02\n",
      "  1.46951273e-01  6.62950426e-02  5.21072447e-02  8.00222009e-02\n",
      " -8.18094850e-06 -3.02801467e-02 -4.92367335e-02  1.94570404e-02\n",
      " -9.94136557e-02  8.83008912e-02  3.84200886e-02  7.44039118e-02\n",
      "  3.64196636e-02 -1.32087246e-02 -2.56350022e-02  5.92893548e-02\n",
      " -5.40583692e-02  2.14318205e-02 -2.11051628e-02 -1.43098123e-02\n",
      " -1.19157443e-02 -5.54598607e-02 -2.16045287e-02  3.39123537e-04\n",
      "  4.19097804e-02 -1.60611153e-03  3.53510715e-02  3.52155901e-02\n",
      " -6.34447560e-02 -4.81643826e-02  3.13801989e-02 -7.16898739e-02\n",
      "  3.82659845e-02 -6.40602559e-02 -4.94452789e-02  4.93990369e-02\n",
      "  1.12332419e-01 -4.90587614e-02 -3.10008116e-02 -1.68319531e-02\n",
      "  6.98777437e-02 -3.37676294e-02  4.40786481e-02  5.82311116e-02\n",
      "  9.28310603e-02 -6.55981824e-02 -2.85188928e-02 -1.78808607e-02\n",
      " -2.42599230e-02 -9.54742059e-02  4.41729277e-03  1.16973385e-01\n",
      "  6.51650829e-03 -3.72444317e-02 -3.70308012e-02  1.94207076e-02\n",
      "  3.03879399e-02  2.64709201e-02 -4.22812961e-02  2.51902044e-02\n",
      "  3.14523913e-02  7.33921826e-02 -2.23163441e-02  6.30845800e-02\n",
      "  5.48904836e-02 -2.50942982e-03 -1.13640670e-02 -3.72841768e-02\n",
      "  1.12230740e-02 -5.59561001e-03 -3.37056592e-02  4.61643599e-02\n",
      " -7.00156018e-02 -2.63304375e-02 -9.22688004e-03 -2.19809078e-02\n",
      "  8.93138722e-02 -1.39297368e-02 -2.60721706e-03 -2.19502803e-02\n",
      " -8.00992623e-02 -1.16365859e-02 -1.74199808e-02  3.57605815e-02\n",
      "  2.51780506e-02  1.10489443e-01  2.90333591e-02 -1.19357668e-01\n",
      " -2.64204517e-02  2.18375064e-02 -4.26408164e-02  4.73506842e-03\n",
      "  1.30707230e-02 -9.49456468e-02 -9.93978605e-02 -5.03512658e-03]\n"
     ]
    }
   ],
   "source": [
    "# Experiment\n",
    "# checking for a tweet with words not present in the model vocab\n",
    "temp_1[1288351]= 'hi, i am piyush '\n",
    "test = tweet_to_vec(temp_1, CBOW_Word2Vec)\n",
    "print(test.shape)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JXFl2gM1g9Q"
   },
   "source": [
    "## Vectorizing the data using custom Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUzviCLwxrSR",
    "outputId": "4de3da72-8de6-4971-ebb1-b9f9210d4806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 98.05273222923279\n",
      "Elapsed time is 123.00889182090759\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the X_Train and X_val data using CBOW_Word2Vec\n",
    "start_time = time.time()\n",
    "X_Train_cbow = tweet_to_vec(X_Train,CBOW_Word2Vec)\n",
    "print(f\"Elapsed time is {time.time()-start_time}\")\n",
    "X_val_cbow = tweet_to_vec(X_val,CBOW_Word2Vec)\n",
    "print(f\"Elapsed time is {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WjbWXtKqNLOU"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luCGQHwZRf9z",
    "outputId": "cbebf74c-7114-4a65-bef3-fd5eaff69638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 98.455881357193\n",
      "Elapsed time is 123.42654490470886\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing X_Train and X_val using SKIP_Word2Vec\n",
    "start_time = time.time()\n",
    "\n",
    "X_Train_skip = tweet_to_vec(X_Train,SKIP_Word2Vec)\n",
    "print(f\"Elapsed time is {time.time()-start_time}\")\n",
    "X_val_skip = tweet_to_vec(X_val,SKIP_Word2Vec)\n",
    "\n",
    "print(f\"Elapsed time is {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dj3UZU6PR5xh",
    "outputId": "1d1f159e-2c4e-400a-f57d-14bc4dce480b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X_Train_cbow is (1132291, 300), and X_val_cbow is (283073, 300)\n",
      "Dimensions of X_Train_skip is (1132291, 300), and X_val_skip is (283073, 300)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the X_train_cbow, X_val_cbow\n",
    "print(f\"Dimensions of X_Train_cbow is {X_Train_cbow.shape}, and X_val_cbow is {X_val_cbow.shape}\")\n",
    "\n",
    "# checking the shape of the X_train_cbow, X_val_cbow\n",
    "print(f\"Dimensions of X_Train_skip is {X_Train_skip.shape}, and X_val_skip is {X_val_skip.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "h6VqqxNuR5p_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S4dl9NUTTL4"
   },
   "source": [
    "## Let's start the models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_G59stbTYzk"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8iCFbyiHP8R",
    "outputId": "c32a4d36-b670-4b5c-8a47-459c214f06f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training started\n",
      "Elapsed time 11.43055510520935\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(f\"model training started\")\n",
    "start_time = time.time()\n",
    "model = SGDClassifier(loss = 'hinge', max_iter = 1000, tol = 1e-3)\n",
    "\n",
    "# model training\n",
    "model.fit(X_Train_cbow, y_train)\n",
    "\n",
    "print(f\"Elapsed time {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZWblV2ANHP4G"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val_cbow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmSksxt5HPzj",
    "outputId": "3f0dd3d0-5b04-4c39-84dc-0b76f00dd19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73    145367\n",
      "           1       0.71      0.73      0.72    137706\n",
      "\n",
      "    accuracy                           0.73    283073\n",
      "   macro avg       0.73      0.73      0.73    283073\n",
      "weighted avg       0.73      0.73      0.73    283073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_pred,y_val)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "e-MvrhX7HOza"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "291Bng9eYCNr"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRwMpjXrMDzF",
    "outputId": "7580ce87-8cd4-4184-b54a-48d7d4320d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 290.4871664047241\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72    141726\n",
      "           1       0.72      0.72      0.72    141347\n",
      "\n",
      "    accuracy                           0.72    283073\n",
      "   macro avg       0.72      0.72      0.72    283073\n",
      "weighted avg       0.72      0.72      0.72    283073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(n_estimations = 50, max_depth = 10, tres_method = 'hist', n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_clf.fit(X_Train_cbow[:200000], y_train[:200000])\n",
    "print(f\"Elapsed time is {time.time()-start_time}\")\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_val_cbow)\n",
    "\n",
    "report = classification_report(y_pred_xgb,y_val)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJICByi7PTuK"
   },
   "source": [
    " Trying LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8MjhsqEPDVL",
    "outputId": "c9ce237b-9439-4a15-f7bd-cc3364db618c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_Train_cbow, label=y_train)\n",
    "test_data = lgb.Dataset(X_val_cbow, label=y_val, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",  # Change to \"multiclass\" if more than 2 labels\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 31,  # Controls tree complexity\n",
    "    \"max_depth\": -1,  # Auto depth selection\n",
    "    \"n_estimators\": 50,  # Number of boosting rounds\n",
    "    \"subsample\": 0.8,  # Use 80% of data per tree\n",
    "    \"colsample_bytree\": 0.8,  # Use 80% of features per tree\n",
    "    \"verbose\": -1,\n",
    "    \"n_jobs\": -1,  # Use all CPU cores\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Training LightGBM model...\")\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq8spSkjPDRq",
    "outputId": "026a0327-59b0-4fa4-be88-b89980831e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68    149773\n",
      "           1       0.64      0.68      0.66    133300\n",
      "\n",
      "    accuracy                           0.67    283073\n",
      "   macro avg       0.67      0.67      0.67    283073\n",
      "weighted avg       0.67      0.67      0.67    283073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val_cbow)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)  # Convert probabilities to binary classes\n",
    "\n",
    "report = classification_report(y_pred,y_val)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Uh64c2zZlnn"
   },
   "source": [
    "## Using SGDClassifier and Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XW7kNItkZyJ2"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9yHWFmqZx-K"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_Train_cbow, y_train, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val_cbow)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\": [\"hinge\", \"log\"],  # Hinge = SVM, Log = Logistic Regression\n",
    "    \"alpha\": [1e-3, 1e-2],  # Regularization\n",
    "    \"learning_rate\": [ \"optimal\", \"adaptive\"],\n",
    "    \"eta0\": [0.001, 0.01],  # Learning rate step size\n",
    "}\n",
    "\n",
    "sgd = SGDClassifier(max_iter = 1000, tol = 1e-3, random_state = 42)\n",
    "\n",
    "grid_search = GridSearchCV(sgd, param_grid, scoring = 'accuracy', cv = 5, verbose = 2, n_jobs = -1)\n",
    "grid_search.fit(X_train[:500000], y_train[:500000])\n",
    "\n",
    "print(f\"Best parameters are {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Tg9naVdcdHK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
