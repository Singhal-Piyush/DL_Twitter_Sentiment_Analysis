{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Sentiment Analysis\n",
        "\n",
        "## Notebook 3 : Deep Learning comparison of model ANN, LSTM and BERT"
      ],
      "metadata": {
        "id": "Vd5B4aA2NSmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new library download\n",
        "\n"
      ],
      "metadata": {
        "id": "jlPBBpnePUxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnsSI7g1NLYp"
      },
      "outputs": [],
      "source": [
        "# Importing libraires for file and time operations\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Libraries for random number generation and warnings\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Library for splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# libraries for Word2Vec\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# libraries for nltk\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Libraries for Deep Learning with PyTorch\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Library for clearing Jupyter Notebook cell output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Libraries for model evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new library importing inbetween\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CNQX_-VBPUPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seeting seeds for reproducibility\n"
      ],
      "metadata": {
        "id": "LudqHFUdPbsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value):\n",
        "  # set python's random seed\n",
        "  random.seed(seed_value)\n",
        "\n",
        "  # set numpy's random seed\n",
        "  np.random.seed(seed_value)\n",
        "\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "  # set Pytorch;s CPU seed\n",
        "  torch.manual_seed(seed_value)\n",
        "  # Set CuDNN deterministic mode (for further reproducibility)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  # Set PyTorch's GPU seed (if available)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "kE5w-kVZPZ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "gm2G1p1yUsJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy files from google drive to google colab content\n",
        "\n",
        "!cp '/content/drive/MyDrive/Twitter_Sentiment_Analysis/training_processed_data.csv' '/content/'\n",
        "!cp '/content/drive/MyDrive/Twitter_Sentiment_Analysis/SKIP_Word2Vec.model' '/content/'\n",
        "!cp '/content/drive/MyDrive/Twitter_Sentiment_Analysis/CBOW_Word2Vec.model' '/content/'"
      ],
      "metadata": {
        "id": "ysVvlgMJUrUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "dataset = pd.read_csv('training_processed_data.csv',nrows = 500000)\n",
        "dataset.sample(10)\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XHCqbzHVfPT",
        "outputId": "b4f78868-1273-48fe-d009-a8585d351c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dropna(axis = 0, how = \"any\", inplace = True)\n",
        "dataset.isnull().sum()\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmPTqWP2lh9g",
        "outputId": "bbbf37bb-0fdc-4a29-8c08-d3262ec12834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(492950, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the data into Train and Test data"
      ],
      "metadata": {
        "id": "U8UUeUCxVqxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into Training and Testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['target'], test_size = 0.2, random_state = 42)\n"
      ],
      "metadata": {
        "id": "ptT6U37mVqa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Vectorization"
      ],
      "metadata": {
        "id": "oahYrpofWQB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library to work with Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Define the path to the pre-trained Word2Vec models\n",
        "cbow_path = '/content/CBOW_Word2Vec.model'\n",
        "sg_path = '/content/SKIP_Word2Vec.model'\n",
        "\n",
        "# Load the models in the memory\n",
        "cbow = Word2Vec.load(cbow_path)\n",
        "sg = Word2Vec.load(sg_path)"
      ],
      "metadata": {
        "id": "QtTNoPeRWO9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting the raw data into vectorize format with 300 dimensions using the above mentioned models"
      ],
      "metadata": {
        "id": "IYQrDVA8XfVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "def vectorizer(text, word2vec_model):\n",
        "  x = len(text)\n",
        "  y = 300\n",
        "\n",
        "  matrix = np.zeros((x,y))\n",
        "\n",
        "  # iterate through each text sample in the dataframe\n",
        "  for i in range(x):\n",
        "    words = WhitespaceTokenizer().tokenize(text.iloc[i])\n",
        "\n",
        "    for word in words:\n",
        "      word_vec = []\n",
        "      if word in word2vec_model.wv:\n",
        "        word_vec.append(word2vec_model.wv.get_vector(word))\n",
        "\n",
        "      if word_vec:\n",
        "        matrix[i] = np.mean(word_vec, axis=0)\n",
        "\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "YN8P-VWYXk_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the data vectorization using CBOW\n",
        "start_time = time.time()\n",
        "\n",
        "matrix_train_cbow = vectorizer(X_train,cbow)\n",
        "matrix_test_cbow = vectorizer(X_test,cbow)\n",
        "\n",
        "print(f\" Data Vectorization completed using cbow. Duration {time.time()-start_time} secs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIQuj2aPgqNI",
        "outputId": "97e0989c-703f-400b-b7c0-c2ee9d72a291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data Vectorization completed using cbow. Duration 60.00106453895569 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrix_train_cbow.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-x_cTI4-yzg",
        "outputId": "cde3abbd-baad-4f9f-eab1-fa65abfdcfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(394360, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrix_train_cbow[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRoDxSUTB1DR",
        "outputId": "5030ffb3-75e8-45bc-f7d2-3bcdbe6a738d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.21310890e+00 -1.59038031e+00 -1.03793286e-01 -2.09765494e-01\n",
            "  7.06457794e-01  2.22103342e-01 -5.52951515e-01  4.01179976e-04\n",
            " -6.63659871e-01 -2.86419392e-01  3.09581041e-01 -3.68017524e-01\n",
            " -1.61698079e+00  8.46598744e-01  8.76871884e-01  3.49490732e-01\n",
            " -1.74440742e+00  1.36529773e-01  2.74539739e-01  3.04339767e-01\n",
            "  2.77809709e-01  2.94365525e-01  5.00815809e-01 -2.77122855e-01\n",
            "  8.25947523e-01 -7.64158666e-01 -7.90861726e-01 -5.97459435e-01\n",
            "  2.11164296e-01  7.01822698e-01  3.26846004e-01 -1.76645517e+00\n",
            "  1.03135669e+00  7.50023603e-01  7.44568288e-01  2.31459305e-01\n",
            "  2.76142269e-01  7.34570742e-01 -1.49256217e+00 -7.18964636e-01\n",
            " -6.16045892e-01 -2.79662937e-01  3.34402137e-02  4.45389748e-01\n",
            "  3.39251250e-01  5.00838935e-01  5.81753731e-01 -2.00212336e+00\n",
            "  8.01831186e-01  2.12147146e-01  1.40123576e-01  5.77987731e-01\n",
            " -6.23167813e-01  2.76066780e-01  3.75610590e-01  8.78760040e-01\n",
            " -8.73566389e-01 -1.14821124e+00  6.81660846e-02 -2.12199837e-01\n",
            "  7.23350227e-01  3.88899744e-01 -6.63738549e-01  1.13701808e+00\n",
            " -1.08043917e-01  2.19484240e-01  1.48443878e+00  9.07455206e-01\n",
            " -7.16830969e-01 -3.14814448e-01  1.19575346e+00  1.14097333e+00\n",
            "  7.06851006e-01  1.08587742e+00  9.01404321e-01 -1.25241160e+00\n",
            "  1.22177672e+00 -9.75096375e-02 -8.42960536e-01  6.95721388e-01\n",
            "  4.71290648e-01  7.92661130e-01 -3.93147953e-02 -9.92410183e-01\n",
            " -1.37608337e+00  1.74468696e-01  4.98221576e-01  4.91909087e-01\n",
            " -7.65683115e-01  2.65911687e-02  3.14168066e-01 -7.50375211e-01\n",
            "  2.95813270e-02 -2.46355399e-01 -8.85947526e-01 -7.45294467e-02\n",
            "  1.20066559e+00 -2.07249790e-01 -3.01125079e-01  7.75642276e-01\n",
            " -1.41160524e+00  7.66632915e-01 -7.81939104e-02  2.91251224e-02\n",
            " -2.48485947e+00  6.88613474e-01 -3.51835012e-01  3.80542427e-02\n",
            " -3.49020623e-02 -5.04562333e-02 -7.49015063e-02 -1.23344851e+00\n",
            "  7.46028051e-02  4.78917748e-01 -1.89910948e+00 -3.38556737e-01\n",
            "  8.62513721e-01 -1.96014605e-02 -1.00042462e+00  6.93678916e-01\n",
            "  2.29358539e-01 -1.34096220e-01 -7.52491534e-01  3.30291897e-01\n",
            " -1.44190776e+00  1.33795187e-01 -3.19550812e-01 -1.28481102e+00\n",
            " -1.22038789e-01 -9.46038738e-02 -6.59474313e-01 -8.07783365e-01\n",
            " -2.16922235e+00 -8.96352902e-02 -5.78039110e-01  1.84625244e+00\n",
            " -9.73825097e-01 -9.61208995e-03 -1.08299911e+00 -4.95105162e-02\n",
            "  3.91465612e-02  4.31963354e-01 -1.75524220e-01 -1.39565325e+00\n",
            "  6.29558980e-01  1.62405580e-01  7.00020492e-01 -1.60022056e+00\n",
            " -3.62911612e-01  2.49984473e-01 -6.90507948e-01 -1.32050347e+00\n",
            "  1.32145071e+00 -1.09975910e+00 -4.84497488e-01  9.68514204e-01\n",
            "  6.61174893e-01 -3.71807903e-01 -5.73900044e-01  2.46760026e-01\n",
            "  1.62298381e+00 -6.61305904e-01 -4.32763755e-01 -4.56554353e-01\n",
            " -3.84433985e-01 -3.49678159e-01 -1.63601315e+00  3.42286825e-01\n",
            "  8.45592678e-01 -1.12959549e-01  9.00841534e-01  2.43709087e-01\n",
            "  3.50502938e-01  7.90826827e-02 -3.02531689e-01 -5.38660228e-01\n",
            " -1.20744073e+00  1.23056650e+00 -3.85427564e-01 -1.32238433e-01\n",
            " -1.01472282e+00  8.90564859e-01 -4.34543878e-01 -1.64958775e+00\n",
            "  6.73565328e-01 -5.55002689e-01 -2.61944294e-01 -2.41224058e-02\n",
            "  4.77674812e-01  4.58349466e-01  5.36535323e-01  7.39573121e-01\n",
            " -8.97659361e-02 -1.90855309e-01 -3.68331760e-01 -5.89282990e-01\n",
            " -8.52714717e-01 -1.02246678e+00  1.70739532e-01 -7.42242277e-01\n",
            " -2.76987404e-01 -1.06239736e+00 -1.02092767e+00  1.43782878e+00\n",
            " -2.46716544e-01 -1.03376901e+00  2.62921870e-01  5.45147300e-01\n",
            " -8.91828001e-01 -1.02053261e+00  9.48966622e-01 -3.72110128e-01\n",
            " -2.79993445e-01  5.02133608e-01 -7.02343702e-01 -4.43222076e-01\n",
            "  2.19508689e-02 -2.03591466e-01 -3.27367842e-01  5.28235674e-01\n",
            "  4.35868889e-01  5.02695620e-01 -2.66489476e-01  7.17062950e-01\n",
            "  1.68797001e-01  1.63321063e-01 -1.38816219e-02 -5.01591086e-01\n",
            "  4.39486355e-01  1.22975506e-01 -5.11288166e-01  2.20407438e+00\n",
            "  1.11740363e+00 -2.57961035e-01  1.81738555e-01 -1.49150208e-01\n",
            "  1.10066891e+00 -3.89640965e-02  4.83782798e-01 -8.65192473e-01\n",
            " -1.26411617e-01  8.82759970e-03  1.94776785e+00 -9.85800743e-01\n",
            "  5.94870448e-01  5.94459653e-01 -8.23189616e-01 -8.75585854e-01\n",
            " -6.01327777e-01  3.08773994e-01  2.56109029e-01  6.03656769e-01\n",
            " -2.75804341e-01 -1.19127810e+00 -3.75340343e-01  1.87593684e-01\n",
            "  2.21800968e-01 -1.27511668e+00  4.04858679e-01 -7.25066602e-01\n",
            "  1.23166716e+00  1.01119936e+00 -2.25158453e-01 -9.08953846e-01\n",
            " -7.77240634e-01  4.67536122e-01  2.31453627e-01  4.04585481e-01\n",
            " -3.78004521e-01 -6.45733476e-01 -6.29353821e-01 -1.92700356e-01\n",
            "  9.59624887e-01  5.20037830e-01  6.00585699e-01 -7.47246265e-01\n",
            " -1.60672694e-01 -4.42555487e-01  4.88391757e-01  1.76932663e-01\n",
            " -3.36287826e-01  5.24965823e-01 -5.50007880e-01  3.08970153e-01\n",
            " -1.68636143e+00 -1.14527680e-01  5.13759494e-01 -1.30022788e+00\n",
            "  4.12518829e-01  1.66811383e+00 -6.10033423e-02  4.56459850e-01\n",
            "  1.97571963e-01  8.26627851e-01  1.08540142e+00 -3.19338292e-02\n",
            "  5.10693133e-01 -1.08771652e-01  8.71061265e-01 -3.59989136e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the data vectorization with Skipgram\n",
        "start_time = time.time()\n",
        "\n",
        "matrix_train_sg = vectorizer(X_train,sg)\n",
        "matrix_test_sg = vectorizer(X_test,sg)\n",
        "\n",
        "print(f\"Data Vectorization completed using Skipgram. Duration {time.time()- start_time}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpHXY_EQhLsy",
        "outputId": "2deed560-b8f6-41fe-fc28-31029217e176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Vectorization completed using Skipgram. Duration 204.78982996940613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_awKw-qmRA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the computational device"
      ],
      "metadata": {
        "id": "9duX7Kt0nEzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose GPU if availabel, otherwise use CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(f\"Device being used is {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TZ2GtqJnIJg",
        "outputId": "e4b82d29-6570-472a-ef38-fa805c2b5618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device being used is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5XFpcWbnd2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3H04tMlvn-Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojRTrPs-n9--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First:- Basic Neural Network"
      ],
      "metadata": {
        "id": "EB7eYnien-1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier class sets up the network layer, while train and test functions manage the training and evaluation process."
      ],
      "metadata": {
        "id": "TCqFip-fEYmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network classifier\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(classifier, self).__init__()\n",
        "\n",
        "    # Define hidden layer with ReLU activation\n",
        "    self.features = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
        "\n",
        "    # Define output Layer\n",
        "    self.out = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "  #Define foreard pass\n",
        "  def forward(self, X):\n",
        "    feature = self.features(X)\n",
        "    output = self.out(feature)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "-GPwCg7YoC1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function for a single epoch\n",
        "\n",
        "def train(train_loader, net, epoch, criterion, optimizer):\n",
        "\n",
        "  # set the network to training mode\n",
        "  net.train()\n",
        "\n",
        "  # Initialize list to store batch loss\n",
        "  epoch_loss = []\n",
        "\n",
        "  # Iterate through each batch in the training data loader\n",
        "  for batch in train_loader:\n",
        "    data, target = batch\n",
        "\n",
        "    # Move the data and labels to the chosen device\n",
        "    data , target = data.to(device), target.to(device)\n",
        "\n",
        "    # Forward pass: commute predictions and loss\n",
        "    pred = net(data)\n",
        "    loss = criterion(pred.squeeze(),target.float())\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Convert epoch loss to numpy array and compute mean\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "\n",
        "  return epoch_loss.mean()"
      ],
      "metadata": {
        "id": "soBvImI-G0Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Function for single epoch\n",
        "\n",
        "def test(test_loader, net, epoch, criterion):\n",
        "  # set the network for evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  # Disable gradient calculation for performance calculation\n",
        "  with torch.no_grad():\n",
        "    # Initiate loss list to store the loss\n",
        "    epoch_loss = []\n",
        "\n",
        "    # Iterate through each batch in the test data loader\n",
        "    for batch in test_loader:\n",
        "      data,target = batch\n",
        "\n",
        "      # Move data and target to the chosen device\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # Forward pass: compute predictoins and loss\n",
        "      pred = net(data)\n",
        "      loss = criterion(pred.squeeze(), target.float())\n",
        "      epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "    #Convcer the epoch loss to numpy array and compute the mean\n",
        "    epoch_loss = np.asarray(epoch_loss)\n",
        "\n",
        "    return epoch_loss.mean()\n"
      ],
      "metadata": {
        "id": "5KwlAyhZRHkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to train the neural network\n",
        "\n",
        "def train_nn(matrix_train, matrix_test, args, hidden_size = 32, input_size = 300, output_size = 1):\n",
        "\n",
        "\n",
        "  # converting training and testing matrices to pytorch tensors\n",
        "  X_train = torch.from_numpy(matrix_train).float()\n",
        "  Y_train = torch.from_numpy(y_train.values)\n",
        "  train_data = TensorDataset(X_train,Y_train)\n",
        "\n",
        "  X_test = torch.from_numpy(matrix_test).float()\n",
        "  Y_test = torch.from_numpy(y_test.values)\n",
        "  test_data = TensorDataset(X_test, Y_test)\n",
        "\n",
        "  # Create data Loaders for training and testing datasets\n",
        "  train_loader = DataLoader(train_data, batch_size = args['batch_size'], shuffle = True,\n",
        "                            num_workers = args['num_workers'])\n",
        "  test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = True,\n",
        "                           num_workers = args['num_workers'])\n",
        "\n",
        "  # Initialize the neular network\n",
        "  net = classifier(input_size, hidden_size, output_size)\n",
        "  net.to(device)\n",
        "\n",
        "  #Define the loss function\n",
        "  criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "  # Chooses the optimizer based on user input\n",
        "  if args['optimizer'] == 'Adam':\n",
        "    optimizer = optim.Adam(net.parameters(), lr = args['lr'],\n",
        "                          weight_decay = args['weight_decay'])\n",
        "\n",
        "  # Initializes variable to keep track of losses and other metrics\n",
        "  train_losses, test_losses = [], []\n",
        "  best_test_loss = float('inf')\n",
        "  best_model_epoch = -1\n",
        "  no_improvement_epoch = 0\n",
        "\n",
        "  # Records the time at which training starts\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Main Training of neural network\n",
        "  for epoch in range(args['num_epochs']):\n",
        "\n",
        "    #Clear the output and prints the currnet training status\n",
        "    clear_output(wait = True)\n",
        "\n",
        "    print(\"Training neural network ... Epoch \" + str(epoch) + '/' +str(args['num_epochs']-1))\n",
        "    if best_model_epoch != -1:\n",
        "      print(\"Best test loss epoch: \"+ str(best_model_epoch))\n",
        "\n",
        "    # Train the network and logs the loss\n",
        "\n",
        "    epoch_train_loss = train(train_loader, net, epoch, criterion, optimizer)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # Test the network and logs the loss\n",
        "    epoch_test_loss = test(test_loader, net, epoch, criterion)\n",
        "    test_losses.append(epoch_test_loss)\n",
        "\n",
        "    # Checks for improvement in test loss\n",
        "    diff = best_test_loss - epoch_test_loss\n",
        "    if epoch_test_loss< best_test_loss:\n",
        "      best_test_loss = epoch_test_loss\n",
        "      #Save the state of the best model for future reference\n",
        "      torch.save(net.state_dict(), 'backup_best_model.pth')\n",
        "      best_model_epoch = epoch\n",
        "\n",
        "    # Implement early stopping based on predefined tolerance and patience\n",
        "    if diff> args['tolerance']:\n",
        "      no_improvement_epochs = 0\n",
        "    else:\n",
        "      no_improvement_epochs +=1\n",
        "    if no_improvement_epochs >= args['patience']:\n",
        "      print(\"Early Stopping at epoch: \", epoch)\n",
        "      break\n",
        "\n",
        "    # Record the duration for which the model training took\n",
        "    duration = time.time()-start_time\n",
        "\n",
        "    # Clear the output and prints the final training status\n",
        "    clear_output(wait = True)\n",
        "\n",
        "    print(f\"Neural network training has finished. Elasped time {duration :.2f}\")\n",
        "    print(\"Best model saved\")\n",
        "    print(\"Best test loss epoch: \"+ str(best_model_epoch))\n",
        "\n",
        "    # Load the best model\n",
        "    true_test_labels, predicted_test_labels = [], []\n",
        "    net.load_state_dict(torch.load(\"backup_best_model.pth\"))\n",
        "\n",
        "    # Use the best model to make predictions on the train data\n",
        "    with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "        data, target = batch\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pred = net(data)\n",
        "        true_test_labels.extend(target.cpu().numpy())\n",
        "        predicted_test_labels.extend((torch.sigmoid(pred).squeeze()>0.5).cpu().numpy())\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    test_accuracy = accuracy_score(true_test_labels, predicted_test_labels)\n",
        "    test_precision = precision_score(true_test_labels, predicted_test_labels)\n",
        "    test_recall = recall_score(true_test_labels, predicted_test_labels)\n",
        "    test_f1 = f1_score(true_test_labels, predicted_test_labels)\n",
        "\n",
        "    # print the performance metrics of the best model\n",
        "    print(\"Best model metrics: \")\n",
        "    print(f\"Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 {test_f1}\")\n",
        "\n",
        "\n",
        "  return (train_losses, test_losses, best_model_epoch)"
      ],
      "metadata": {
        "id": "BLu5jWx4Siqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Grid Definition"
      ],
      "metadata": {
        "id": "es2IQAK_2a4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameter grid for hyperparameter tuning\n",
        "\n",
        "param_grid = {\n",
        "    'vectorization' :['cbow'],\n",
        "    'criterion': ['BCEWithLogitsLoss'],\n",
        "    'optimizer' :['Adam'],\n",
        "    'batch_size' : [500],\n",
        "    'num_workers':[2],\n",
        "    'lr' : [0.0001,0.001],\n",
        "    'weight_decay' : [0.0001],\n",
        "    'num_epochs':[500],\n",
        "    'tolerance' : [0.003],\n",
        "    'patience' : [25]\n",
        "}\n",
        "\n",
        "# generate all combinations of hyperparameters for grid search\n",
        "all_combinations = [dict(zip(param_grid,v)) for v in product(*param_grid.values())]"
      ],
      "metadata": {
        "id": "Bb_xv3cu2ZoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_combinations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7SMtsH44p2E",
        "outputId": "2ef15c0a-9a8f-4dcc-afdb-d36da07e707b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# record the start time for model training process\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop throught each combinations of hyperparameters\n",
        "for params in all_combinations:\n",
        "\n",
        "  # Select the appropriate word vector based on vectorization parameter\n",
        "  if params['vectorization']== 'cbow':\n",
        "    matrix_train = matrix_train_cbow.copy()\n",
        "    matrix_test = matrix_test_cbow.copy()\n",
        "  elif params['vectorization']=='sg':\n",
        "    matrix_train = matrix_train_sg.copy()\n",
        "    matrix_test = matrix_test_sg.copy\n",
        "\n",
        "  # Train the neural network with the given parameters\n",
        "  train_losses, test_losses, best_epoch = train_nn(matrix_train, matrix_test, params,\n",
        "                                                   hidden_size = params.get('hidden_size',32),\n",
        "                                                   input_size = params.get('input_size',300),\n",
        "                                                   output_size = params.get('putput_size',1))\n",
        "\n",
        "  # Append the training results to the results list\n",
        "  results.append({\n",
        "      'params':params,\n",
        "      'train_losses':train_losses,\n",
        "      'test_losses' : test_losses,\n",
        "      'best_epoch' : best_epoch\n",
        "  })\n",
        "\n",
        "# record duration for training\n",
        "duration = time.time()-start_time\n",
        "\n",
        "print(f\" Model training is completed. Elapsed time is {duration} secs\")\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "3dGVkXlL9n-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b7db54-cb1a-408c-befa-ca7a53dd1791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training neural network ... Epoch 25/499\n",
            "Best test loss epoch: 17\n",
            "Early Stopping at epoch:  25\n",
            " Model training is completed. Elapsed time is 492.5805265903473 secs\n",
            "[{'params': {'vectorization': 'cbow', 'criterion': 'BCEWithLogitsLoss', 'optimizer': 'Adam', 'batch_size': 500, 'num_workers': 2, 'lr': 0.0001, 'weight_decay': 0.0001, 'num_epochs': 500, 'tolerance': 0.003, 'patience': 25}, 'train_losses': [0.22805305, 0.047322586, 0.0153300725, 0.007410625, 0.004529834, 0.00314678, 0.0023377107, 0.0017929041, 0.0013842217, 0.0010603114, 0.0008020084, 0.0005992995, 0.00044533022, 0.0003341641, 0.00025485325, 0.00019977466, 0.0001620882, 0.00013638068, 0.000119417324, 0.00010826704, 0.000101085265, 9.6412354e-05, 9.3667775e-05, 9.259407e-05, 9.185473e-05, 9.178583e-05, 9.185104e-05, 9.1759444e-05, 9.177482e-05], 'test_losses': [0.08781215, 0.023617854, 0.009860607, 0.0055590617, 0.003668413, 0.0026701924, 0.0020026078, 0.0015546869, 0.0011923359, 0.000912208, 0.00068267004, 0.0005106336, 0.000380244, 0.00028782245, 0.00022296845, 0.00017815521, 0.00014709192, 0.00012653848, 0.00011240797, 0.00010363144, 9.804855e-05, 9.539385e-05, 9.449142e-05, 9.157855e-05, 9.213387e-05, 9.236355e-05, 9.2646536e-05, 9.265867e-05, 9.281905e-05], 'best_epoch': 23}, {'params': {'vectorization': 'cbow', 'criterion': 'BCEWithLogitsLoss', 'optimizer': 'Adam', 'batch_size': 500, 'num_workers': 2, 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'tolerance': 0.003, 'patience': 25}, 'train_losses': [0.034077685, 0.0014610186, 0.00067699567, 0.00038059178, 0.0002514683, 0.0001873346, 0.00015169509, 0.00012949585, 0.00011482747, 0.00010470315, 9.818267e-05, 9.44547e-05, 9.248793e-05, 9.172268e-05, 9.159564e-05, 9.132246e-05, 9.130602e-05, 9.140705e-05, 9.123148e-05, 9.128068e-05, 9.128879e-05, 9.128132e-05, 9.131362e-05, 9.130699e-05, 9.1291295e-05, 9.135088e-05], 'test_losses': [0.0022353781, 0.0009251146, 0.00048056626, 0.0002973221, 0.00021269103, 0.00016677463, 0.00013748184, 0.00012120928, 0.00010929209, 0.00010226695, 9.4849674e-05, 9.361278e-05, 9.303141e-05, 9.3118986e-05, 9.1714355e-05, 9.3424984e-05, 9.20903e-05, 8.490861e-05, 9.376674e-05, 9.1236725e-05, 9.166492e-05, 9.0185364e-05, 9.315522e-05, 9.0414615e-05, 9.518326e-05, 8.5919666e-05], 'best_epoch': 17}]\n"
          ]
        }
      ]
    }
  ]
}